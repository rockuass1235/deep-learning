# 過擬合與欠擬合

## 過擬合(overfitting)
過擬合是一種現象。當我們提高在訓練數據上的表現時，在測試數據上反而下降，這就被稱為過擬
合，或過配。
例如:
表現在訓練數據上的誤差非常小，而在測試數據上誤差反而增大。其原因一般是模型過於復雜，過分得去擬合
數據的噪聲和outliers.正則化則是對模型參數添加先驗，使得模型複雜度較小，對於噪聲以及outliers的輸入擾動相對較小。

過擬合發生的本質原因，是由於監督學習問題的不適定：在高中數學我們知道，從n個（線性無關）方程可以解n個變量，解n+1個變量就會解不出。在監督學習中，數據遠遠少於模型空間（變量龐大）
，那麼就會發生過擬合。
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/overfit.png)

過擬合可以分解成以下2點：
* 有限的訓練數據不能完全反映出一個模型的好壞，然而我們卻不得不在這有限的數據上挑選模型，因此我們完全有可能挑選到在訓練數據上表現很好而在測試數據上表現很差的模型，因為我
們完全無法知道模型在測試數據上的表現。

* 如果模型空間很大，也就是有很多很多模型可以給我們挑選，那麼挑到對的模型的機會就會很小。

過度擬合的問題通常發生在變量（特徵）過多的時候。這種情況下訓練出的方程總是能很好的擬合訓練數據，也就是說，我們的代價函數可能非常接近於0 或者就為0。
但是，這樣的曲線千方百計的去擬合訓練數據，這樣會導致它無法泛化到新的數據樣本中，以至於無法預測新樣本價格。

## **如果沒足夠的數據集去約束變量過多的模型，那麼就會發生過擬合。防止過擬合的目的是為了讓模型的生命更長久，把它扔到現實的數據海洋中活得好，活得久。**
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/overfit2.png)
---

## 解決方法一dropout
採用dropout方法。這個方法在神經網絡裡面很常用。dropout方法是ImageNet中提出的一種方法，通俗一點講就是dropout方法在訓練的時候讓神經元以一定的概率不工作。具體看下圖：
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/dropout.png)


## 解決方法一正則化(Regularization)
最簡單的解釋就是加了先驗。在數據少的時候，先驗知識可以防止過擬合。
通常正則化方向依據我們的知識給予的假設方向而定

舉2個例子：
* 拋硬幣，推斷正面朝上的概率。如果只能拋5次，很可能5次全正面朝上，這樣你就得出錯誤的結論：正面朝上的概率是1--------過擬合！如果你在模型裡加正面朝上概率是0.5的先驗，結果就不會那麼離譜。這其實就是正則。

 * 最小二乘回歸問題：加2範數正則等價於加了高斯分佈的先驗，加1範數正則相當於加拉普拉斯分佈先驗。通常我們最常用2norm
### L2範數
範數（英語：norm），是具有「長度」概念的函數。在線性代數、泛函分析及相關的數學領域，是一個函數，其為向量空間內的所有向量賦予非零的正長度或大小。另一方面，半範數（英語：seminorm）可以為非零的向量賦予零長度。<br>
<br>


理論上，loss function 的值越小越好，因為你希望模型越準確越好，可惜在訓練時，你的資料只是一小撮。error 非常小，你就 overfit 、無法處理未來的一大堆測試資料集。Loss function 加入懲罰項，你就沒那麼準。懲罰項的係數是重點。係數太小，甚至是0，你等於麼都沒做

懲罰項為 w^2，會避免 w 太大。為什麼？因為 Loss function 的值要越來越小，w 當然也要傾向於越來越小。

```Python
def l2norm(w, b):
    return (w**2).sum() + b**2

```

## 權重衰減(weight decay)

我們觀察了過擬合現象，即模型的訓練誤差遠小於它在測試集上的誤差。雖然增大訓練數據集可能會減輕過擬合，但是獲取額外的訓練數據往往代價高昂。本節介紹應對過擬合問題的常用方法：權重衰減（weight decay）。

在加入正則化的loss function 會變成 loss + decay * l2norm(w,b)，其中超參數 decay > 0<br>
當權重參數均為0時，懲罰項最小。當 λ 較大時，懲罰項在損失函數中的比重較大，這通常會使學到的權重參數的元素較接近0。當 λ 設為0時，懲罰項完全不起作用。

可見， L2 範數正則化令權重 w1 和 w2 先自乘小於1的數，再減去不含懲罰項的梯度。因此， L2 範數正則化又叫權重衰減。

權重衰減通過懲罰絕對值較大的模型參數為需要學習的模型增加了限制，這可能對過擬合有效。實際場景中，我們有時也在懲罰項中添加偏差元素的平方和。

```Python

decay = 0.001
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr, 'wd': decay})

```




## 欠擬合(underfitting)

如果我們要在訓練數據上表現良好，最為直接的方法就是要在足夠大的模型空間中挑選模型，否則如果模型空間很小，就不存在能夠擬合數據很好的模型。
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/underfit.png)


## 附件: 參閱reg-scratch, reg-scratch-gluon
