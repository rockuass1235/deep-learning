# 深度學習的activation function


**優先使用ReLU (Rectified Linear Unit) 函數作為神經元的activation function**


## 理論

深度學習的基本原理是基於人工神經網絡，信號從一個神經元進入，經過**非線性的** activation function，傳入到下一層神經元；<br>
信號從一個神經元傳入到下一層神經元之前是通過線性疊加來計算的，而進入下一層神經元需要經過非線性的激活函數，繼續往下傳遞，如此循環下去。由於這些非線性函數的反复疊加，才使得神經網絡有足夠的capacity來抓取複雜的特徵。<br>

顯而易見，activation function在深度學習中舉足輕重，也是很活躍的研究領域之一。<br>
目前來講，**選擇怎樣的activation function不在於它能否模擬真正的神經元，而在於能否便於優化整個深度神經網絡。**
下面我們簡單聊一下各類函數的特點以及為什麼現在優先推薦ReLU函數。<br>

## 為什麼要使用非線性激活函數
如果不使用激活函數，這種情況下每一層輸出都是上一層輸入的線性函數。無論神經網絡有多少層，輸出都是輸入的線性函數，這樣就和只有一個隱藏層的效果是一樣的。這種情況相當於多層感知機(MLP)。


## 非線性系統
在物理科學中，如果描述某個系統的方程式其輸入（自變數）與輸出（應變數）不成正比，則稱為非線性系統。由於自然界中大部分的系統本質上都是非線性的，因此許多工程師、物理學家、數學家和其他科學家對於非線性問題的研究都極感興趣。非線性系統和線性系統最大的差別在於，非線性系統可能會導致混沌、不可預測，或是不直觀的結果。

## 什麼是線性的網絡
如果把線性網絡看成一個大的矩陣M。那麼輸入樣本A和B，則會經過同樣的線性變換MA，MB（這裡A和B經歷的線性變換矩陣M是一樣的）。<br>
的確對於單一的樣本A，經過由relu激活函數所構成神經網絡，其過程確實可以等價是經過了一個線性變換M1，但是對於樣本B，在經過同樣的網絡時，由於每個神經元是否激活（0或者Wx+b）與樣本A經過時情形不同了（不同樣本），因此B所經歷的線性變換M2並不等於M1。因此，relu構成的神經網絡雖然對每個樣本都是線性變換，但是不同樣本之間經歷的線性變換M並不一樣，所以整個樣本空間在經過relu構成的網絡時其實是經歷了非線性變換的。


##  Activation function

### Sigmoid

![image](https://github.com/rockuass1235/deep-learning/blob/master/images/sigmoid.png)<br>

傳統神經網絡中最常用的兩個激活函數，Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid）被視為神經網絡的核心所在。<br>

從數學上來看，**非線性的Sigmoid函數對中央區的信號增益較大，對兩側區的信號增益小**，在信號的特徵空間映射上，有很好的效果。<br>

從神經科學上來看，中央區酷似神經元的興奮態，兩側區酷似神經元的抑制態，因而在神經網絡學習方面，可以將重點特徵推向中央區，將非重點特徵推向兩側區。<br>

### 評語

Sigmoid函數是深度學習領域開始時使用頻率最高的activation function。它是便於求導的平滑函數，sigmoid函數，把定義在正無窮到負無窮上的點，映射到0到1之間，當隱藏層很多的時候，往往在網絡訓練的過程中，接近輸出層的隱藏層權值矩陣很快就會收斂，這是優點。然而遠離輸出層的那些卻很難收斂，原因就是sigmoid函數在後向傳遞loss的時候，loss經過sigmoid函數，一次次地進行了衰減，導致那些層的權值調整很小，而無法達到擬合。最後，當隱藏層增加越多，深度學習的效果就越差。這是早期研究深度學習的一個瓶頸。<br>

Sigmoid有三大缺點：

#### Gradient Vanishing
優化神經網絡的方法是Back Propagation，即導數的後向傳遞：先計算輸出層對應的loss，然後將loss以導數的形式不斷向上一層網絡傳遞，修正相應的參數，達到降低loss的目的。<br>
Sigmoid函數在深度網絡中常常會導致導數逐漸變為0，使得參數無法被更新，神經網絡無法被優化。原因在於兩點：

* (1)在上圖中容易看出，當sigmoid(x)中 x 較大或較小時，導數接近0，而後向傳遞的數學依據是微積分求導的鍊式法則，當前層的導數需要之前各層導數的乘積，幾個小數的相乘，結果會很接近0 

* (2) Sigmoid導數的最大值是0.25，這意味著導數在每一層至少會被壓縮為原來的1/4，通過兩層後被變為1/16，…，通過10層後為1/1048576。請注意這裡是“至少”，導數達到最大值這種情況還是很少見的。


#### 輸出不是zero-centered
Sigmoid函數的輸出值恆大於0，這會導致模型訓練的收斂速度變慢。舉例來講下圖紅色箭頭所示的階梯式更新，這顯然並非一個好的優化路徑。深度學習往往需要大量時間來處理大量數據，模型的收斂速度是尤為重要的。所以，總體上來講，訓練深度學習網絡盡量使用zero-centered數據(可以經過數據預處理實現)和zero-centered輸出。
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/zero_center.jpg)
#### 冪運算相對耗時
相對於前兩項，這其實並不是一個大問題，我們目前是具備相應計算能力的，但面對深度學習中龐大的計算量，最好是能省則省:-)。之後我們會看到，在ReLU函數中，需要做的僅僅是一個thresholding，相對於冪運算來講會快很多。
