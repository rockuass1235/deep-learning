# 深度學習的activation function


**優先使用ReLU (Rectified Linear Unit) 函數作為神經元的activation function**


## 理論

深度學習的基本原理是基於人工神經網絡，信號從一個神經元進入，經過**非線性的** activation function，傳入到下一層神經元；<br>
信號從一個神經元傳入到下一層神經元之前是通過線性疊加來計算的，而進入下一層神經元需要經過非線性的激活函數，繼續往下傳遞，如此循環下去。由於這些非線性函數的反复疊加，才使得神經網絡有足夠的capacity來抓取複雜的特徵。<br>

顯而易見，activation function在深度學習中舉足輕重，也是很活躍的研究領域之一。<br>
目前來講，**選擇怎樣的activation function不在於它能否模擬真正的神經元，而在於能否便於優化整個深度神經網絡。**
下面我們簡單聊一下各類函數的特點以及為什麼現在優先推薦ReLU函數。<br>

## 為什麼要使用非線性激活函數
如果不使用激活函數，這種情況下每一層輸出都是上一層輸入的線性函數。無論神經網絡有多少層，輸出都是輸入的線性函數，這樣就和只有一個隱藏層的效果是一樣的。這種情況相當於多層感知機(MLP)。


## 非線性系統
在物理科學中，如果描述某個系統的方程式其輸入（自變數）與輸出（應變數）不成正比，則稱為非線性系統。由於自然界中大部分的系統本質上都是非線性的，因此許多工程師、物理學家、數學家和其他科學家對於非線性問題的研究都極感興趣。非線性系統和線性系統最大的差別在於，非線性系統可能會導致混沌、不可預測，或是不直觀的結果。

## 什麼是線性的網絡
如果把線性網絡看成一個大的矩陣M。那麼輸入樣本A和B，則會經過同樣的線性變換MA，MB（這裡A和B經歷的線性變換矩陣M是一樣的）。<br>
的確對於單一的樣本A，經過由relu激活函數所構成神經網絡，其過程確實可以等價是經過了一個線性變換M1，但是對於樣本B，在經過同樣的網絡時，由於每個神經元是否激活（0或者Wx+b）與樣本A經過時情形不同了（不同樣本），因此B所經歷的線性變換M2並不等於M1。<br>

##  Activation function

### Sigmoid

![image](https://github.com/rockuass1235/deep-learning/blob/master/images/sigmoid.png)<br>

傳統神經網絡中最常用的兩個激活函數，Sigmoid系（Logistic-Sigmoid、Tanh-Sigmoid）被視為神經網絡的核心所在。<br>

從數學上來看，**非線性的Sigmoid函數對中央區的信號增益較大，對兩側區的信號增益小**，在信號的特徵空間映射上，有很好的效果。<br>

從神經科學上來看，中央區酷似神經元的興奮態，兩側區酷似神經元的抑制態，因而在神經網絡學習方面，可以將重點特徵推向中央區，將非重點特徵推向兩側區。<br>

### 評語

Sigmoid函數是深度學習領域開始時使用頻率最高的activation function。它是便於求導的平滑函數，sigmoid函數，把定義在正無窮到負無窮上的點，映射到0到1之間，當隱藏層很多的時候，往往在網絡訓練的過程中，接近輸出層的隱藏層權值矩陣很快就會收斂，這是優點。然而遠離輸出層的那些卻很難收斂，原因就是sigmoid函數在後向傳遞loss的時候，loss經過sigmoid函數，一次次地進行了衰減，導致那些層的權值調整很小，而無法達到擬合。最後，當隱藏層增加越多，深度學習的效果就越差。這是早期研究深度學習的一個瓶頸。<br>

Sigmoid有三大缺點：

#### Gradient Vanishing
優化神經網絡的方法是Back Propagation，即導數的後向傳遞：先計算輸出層對應的loss，然後將loss以導數的形式不斷向上一層網絡傳遞，修正相應的參數，達到降低loss的目的。<br>
Sigmoid函數在深度網絡中常常會導致導數逐漸變為0，使得參數無法被更新，神經網絡無法被優化。原因在於兩點：

* (1)在上圖中容易看出，當sigmoid(x)中 x 較大或較小時，導數接近0，而後向傳遞的數學依據是微積分求導的鍊式法則，當前層的導數需要之前各層導數的乘積，幾個小數的相乘，結果會很接近0 

* (2) Sigmoid導數的最大值是0.25，這意味著導數在每一層至少會被壓縮為原來的1/4，通過兩層後被變為1/16，…，通過10層後為1/1048576。請注意這裡是“至少”，導數達到最大值這種情況還是很少見的。


#### 輸出不是zero-centered
Sigmoid函數的輸出值恆大於0，這會導致模型訓練的收斂速度變慢。舉例來講下圖紅色箭頭所示的階梯式更新，這顯然並非一個好的優化路徑。深度學習往往需要大量時間來處理大量數據，模型的收斂速度是尤為重要的。所以，總體上來講，訓練深度學習網絡盡量使用zero-centered數據(可以經過數據預處理實現)和zero-centered輸出。
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/zero_center.jpg)
#### 冪運算相對耗時
相對於前兩項，這其實並不是一個大問題，我們目前是具備相應計算能力的，但面對深度學習中龐大的計算量，最好是能省則省:-)。之後我們會看到，在ReLU函數中，需要做的僅僅是一個thresholding，相對於冪運算來講會快很多。


### tanh

![image](https://github.com/rockuass1235/deep-learning/blob/master/images/tanh.png)

### 評語
tanh讀作Hyperbolic Tangent，如上圖所示，它解決了zero-centered的輸出問題，然而，gradient vanishing的問題和冪運算的問題仍然存在。

### 近似生物神經激活函數：ReLu

![image](https://github.com/rockuass1235/deep-learning/blob/master/images/relu.png)

2001年，神經科學家Dayan、Abott從生物學角度，模擬出了腦神經元接受信號更精確的激活模型Softplus <br>
同年，Charles Dugas等人在NIPS會議論文中又調侃了一句，Softplus可以看作是強制非負校正函數max ( 0 , x )<br>
偶然的是，同是2001年，ML領域的Softplus/Rectifier激活函數與神經科學領域的提出腦神經元激活頻率函數有神似的地方，這促成了新的激活函數的研究。<br>
<br>

ReLu雖然在大於0的區間是線性的，在小於等於0的部分也是線性的，但是它整體不是線性的，因為不是一條直線，依然**屬於非線性函數。**<br>
<br>
但是，這類激活函數主要用在深度學習裡。**相對於淺層的機器學習，比如經典的三層神經網絡，用它作為激活函數的話，那表現出來的性質肯定是線性的。**但是在深度學習裡，少則幾十，多則上千的隱藏層，雖然，單獨的隱藏層是線性的，但是**很多的隱藏層表現出來的就是非線性的**。這麼說吧，一條直線，是線性的，但是曲線可以看成什麼，是由很多微小的線段組成的。<br>
還有一種解釋就是，不同樣本的同一個feature，在通過relu構成的神經網絡時，流經的路徑不一樣（relu激活值為0，則堵塞；激活值為本身，則通過），因此最終的輸出空間其實是輸入空間的非線性變換得來的。<br>


### 評語

ReLU函數其實就是一個取最大值函數，注意這並不是全區間可導的，但是我們可以取sub-gradient，如上圖所示。ReLU雖然簡單，但卻是近幾年的重要成果，有以下幾大優點：
解決了gradient vanishing問題(在正區間)
計算速度非常快，只需要判斷輸入是否大於0
收斂速度遠快於sigmoid和tanh

ReLU也有幾個需要特別注意的問題：
* ReLU的輸出不是zero-centered
* Dead ReLU Problem，指的是某些神經元可能永遠不會被激活，導致相應的參數永遠不能被更新。有兩個主要原因可能導致這種情況產生: (1) 非常不幸的參數初始化，這種情況比較少見(2) learning rate太高導致在訓練過程中參數更新太大，不幸使網絡進入這種狀態。解決方法是可以採用Xavier初始化方法，以及避免將learning rate設置太大或使用adagrad等自動調節learning rate的算法。<br>

儘管存在這兩個問題，ReLU目前仍是最常用的activation function，在搭建人工神經網絡的時候推薦優先嘗試！

