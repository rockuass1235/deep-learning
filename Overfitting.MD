# 過擬合與欠擬合

## 過擬合(overfitting)
過擬合是一種現象。當我們提高在訓練數據上的表現時，在測試數據上反而下降，這就被稱為過擬
合，或過配。
例如:
表現在訓練數據上的誤差非常小，而在測試數據上誤差反而增大。其原因一般是模型過於復雜，過分得去擬合
數據的噪聲和outliers.正則化則是對模型參數添加先驗，使得模型複雜度較小，對於噪聲以及outliers的輸入擾動相對較小。

過擬合發生的本質原因，是由於監督學習問題的不適定：在高中數學我們知道，從n個（線性無關）方程可以解n個變量，解n+1個變量就會解不出。在監督學習中，數據遠遠少於模型空間（變量龐大）
，那麼就會發生過擬合。
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/overfit.png)

過擬合可以分解成以下2點：
* 有限的訓練數據不能完全反映出一個模型的好壞，然而我們卻不得不在這有限的數據上挑選模型，因此我們完全有可能挑選到在訓練數據上表現很好而在測試數據上表現很差的模型，因為我
們完全無法知道模型在測試數據上的表現。

* 如果模型空間很大，也就是有很多很多模型可以給我們挑選，那麼挑到對的模型的機會就會很小。

過度擬合的問題通常發生在變量（特徵）過多的時候。這種情況下訓練出的方程總是能很好的擬合訓練數據，也就是說，我們的代價函數可能非常接近於0 或者就為0。
但是，這樣的曲線千方百計的去擬合訓練數據，這樣會導致它無法泛化到新的數據樣本中，以至於無法預測新樣本價格。

## **如果沒足夠的數據集去約束變量過多的模型，那麼就會發生過擬合。防止過擬合的目的是為了讓模型的生命更長久，把它扔到現實的數據海洋中活得好，活得久。**
![image](https://github.com/rockuass1235/deep-learning/blob/master/images/overfit2.png)
---

## 解決方法一dropout
採用dropout方法。這個方法在神經網絡裡面很常用。dropout方法是ImageNet中提出的一種方法，通俗一點講就是dropout方法在訓練的時候讓神經元以一定的概率不工作。具體看下圖：
